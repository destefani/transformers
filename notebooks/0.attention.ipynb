{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "\n",
    "Paper: [Attention Is All You Need. Vaswani et al 2017](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "$$ \n",
    "Attention (Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "<img src=\"assets/scaled_dotptoduct_attention.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([1, 2, 10, 10])\n",
      "Key shape: torch.Size([1, 2, 10, 10])\n",
      "Value shapetorch.Size([1, 2, 10, 10])\n",
      "Key transposed shape torch.Size([1, 2, 10, 10])\n",
      "QK^t shape: torch.Size([1, 2, 10, 10])\n",
      "Attention weights shape: torch.Size([1, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_heads = 2\n",
    "seq_leng_q = 10\n",
    "d_k = 10\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "key = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "value = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape{value.shape}\")\n",
    "\n",
    "print(f\"Key transposed shape {key.transpose(-2, -1).shape}\")\n",
    "\n",
    "scores = torch.matmul(query, key.transpose(-2, -1)) / d_k ** 0.5\n",
    "print(f\"QK^t shape: {scores.shape}\")\n",
    "\n",
    "attention_weights = nn.Softmax(dim=-1)(scores)\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2, 10, 10])\n",
      "Attention weights shape: torch.Size([1, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Tensor of shape (batch_size, num_heads, seq_len_q, d_k)\n",
    "            key: Tensor of shape (batch_size, num_heads, seq_len_k, d_k)\n",
    "            value: Tensor of shape (batch_size, num_heads, seq_len_v, d_v) \n",
    "                   Typically, seq_len_k = seq_len_v\n",
    "            mask: Tensor of shape (batch_size, 1, 1, seq_len_k)\n",
    "        Returns:\n",
    "            output: Attention values of shape (batch_size, num_heads, seq_len_q, d_v)\n",
    "            attention_weights: Tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the dot products\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        # Apply the mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Compute the attention weights\n",
    "        attention_weights = self.softmax(scores)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Compute the output\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "scaled_dotproduct_attention = ScaledDotProductAttention(d_k)\n",
    "output, attention_weights = scaled_dotproduct_attention.forward(query, key, value)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Multi-Head Attention\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" width=\"300\" height=\"400\">\n",
    "\n",
    "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 20, 10])\n",
      "Attention weights shape: torch.Size([1, 2, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism as described in the \"Attention Is All You Need\" paper.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the multi-head attention layer. \n",
    "                       It should be a multiple of num_heads.\n",
    "        num_heads (int): The number of parallel attention layers, or \"heads\".\n",
    "        dropout (float, optional): Dropout rate for the attention weights. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        d_model (int): The dimension of the input and output.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_k (int): Dimension of the key, query, and value for each head.\n",
    "        W_q (nn.Linear): Linear transformation for the query.\n",
    "        W_k (nn.Linear): Linear transformation for the key.\n",
    "        W_v (nn.Linear): Linear transformation for the value.\n",
    "        W_o (nn.Linear): Linear transformation for the output.\n",
    "        attention (ScaledDotProductAttention): The scaled dot product attention mechanism.\n",
    "        dropout (nn.Dropout): Dropout layer for the attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0 # d_model must be divisible by num_heads\n",
    "\n",
    "        self.d_model = d_model          # d_model is the dimension of the model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # d_k is the dimension of the keys and values\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(self.d_k, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension of tensor x into (num_heads, d_k).\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            batch_size (int): Batch size.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Reshaped tensor of shape (batch_size, num_heads, seq_len, d_k).\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor of shape (batch_size, seq_len_q, d_model).\n",
    "            key (torch.Tensor): Key tensor of shape (batch_size, seq_len_k, d_model).\n",
    "            value (torch.Tensor): Value tensor of shape (batch_size, seq_len_v, d_model).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len_k).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len_q, d_model).\n",
    "            torch.Tensor: Attention weights tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k).\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear transformations\n",
    "        query = self.W_q(query)\n",
    "        key = self.W_k(key)\n",
    "        value = self.W_v(value)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # Scale the dot product attention\n",
    "        output, attention_weights = self.attention.forward(query, key, value, mask)\n",
    "\n",
    "        # Concatenate heads and transform\n",
    "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "multihead_attention = MultiHeadAttention(d_model=10, num_heads=num_heads)\n",
    "output, attention_weights = multihead_attention.forward(query, key, value)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "<img src=\"assets/encoder.png\" width=\"250\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single encoder layer in the Transformer architecture.\n",
    "    \n",
    "    Each layer consists of:\n",
    "    1. Multi-head self-attention mechanism.\n",
    "    2. Position-wise feed-forward network.\n",
    "    \n",
    "    Additionally, each of these components is surrounded by a residual connection \n",
    "    followed by layer normalization.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the encoder layer.\n",
    "        num_heads (int): Number of attention heads for the multi-head attention mechanism.\n",
    "        d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
    "        dropout (float, optional): Dropout rate for the attention weights and feed-forward network. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        multihead_attention (MultiHeadAttention): The multi-head attention mechanism.\n",
    "        feed_forward (nn.Sequential): The position-wise feed-forward network.\n",
    "        norm1 (nn.LayerNorm): Layer normalization for the attention mechanism's output.\n",
    "        norm2 (nn.LayerNorm): Layer normalization for the feed-forward network's output.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        # Multi-head attention\n",
    "        attention_output, _ = self.multihead_attention.forward(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # Feed-forward network\n",
    "        feed_forward_output = self.feed_forward.forward(x)\n",
    "        return self.norm2(x + self.dropout(feed_forward_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents the encoder component of the Transformer architecture.\n",
    "    \n",
    "    The encoder consists of a stack of identical layers, where each layer has \n",
    "    a multi-head self-attention mechanism and a position-wise feed-forward network.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the encoder.\n",
    "        num_heads (int): Number of attention heads for the multi-head attention mechanism.\n",
    "        d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
    "        num_layers (int): Number of identical layers in the encoder.\n",
    "        dropout (float, optional): Dropout rate for the attention weights and feed-forward network. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): List of encoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([1, 1, 10, 10])\n",
      "Key shape: torch.Size([1, 1, 10, 10])\n",
      "Value shapetorch.Size([1, 1, 10, 10])\n",
      "Output shape: torch.Size([1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_heads = 1\n",
    "seq_leng_q = 10    # This is the length of the query (q) sequence.\n",
    "d_k = 10           # This is the dimension of the keys (k), queries (q), and values (v) in the scaled dot-product attention mechanism.\n",
    "d_model = 10       # This is the dimension of the embeddings and the input and output size of the Transformer's encoder and decoder layers.\n",
    "d_ff = 10          # This is the dimension of the hidden layer in the feed-forward network.\n",
    "\n",
    "# The relationship between them is often d_k = d_model / num_heads\n",
    "assert d_model % num_heads == 0 # d_model must be divisible by num_heads\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "key = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "value = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape{value.shape}\")\n",
    "\n",
    "encoder = TransformerEncoder(d_model=d_model, num_heads=num_heads, d_ff=d_ff, num_layers=1)\n",
    "output = encoder.forward(query)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a classifier using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based classifier.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the Transformer encoder.\n",
    "        num_heads (int): Number of attention heads for the multi-head attention mechanism.\n",
    "        d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
    "        num_layers (int): Number of identical layers in the encoder.\n",
    "        num_classes (int): Number of target classes for classification.\n",
    "        dropout (float, optional): Dropout rate for the attention weights and feed-forward network. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        encoder (TransformerEncoder): The Transformer encoder.\n",
    "        classifier (nn.Linear): Linear layer for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, num_classes, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "       \"\"\"\n",
    "       Forward pass for the Transformer classifier.\n",
    "       \n",
    "       Args:\n",
    "           x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "           mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len).\n",
    "           \n",
    "       Returns:\n",
    "           torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
    "       \"\"\"\n",
    "       # Get the encoder's output\n",
    "       encoder_output = self.encoder(x, mask)   \n",
    "       return self.classifier(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imput_tensor = torch.rand(batch_size, seq_leng_q, d_model)\n",
    "\n",
    "model = TransformerClassifier(d_model=d_model, num_heads=num_heads, d_ff=d_ff, num_layers=1, num_classes=2)\n",
    "output = model.forward(imput_tensor)\n",
    "\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
