{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "\n",
    "Paper: [Attention Is All You Need. Vaswani et al 2017](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "$$ \n",
    "Attention (Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "<img src=\"assets/scaled_dotptoduct_attention.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([1, 2, 10, 10])\n",
      "Key shape: torch.Size([1, 2, 10, 10])\n",
      "Value shapetorch.Size([1, 2, 10, 10])\n",
      "Key transposed shape torch.Size([1, 2, 10, 10])\n",
      "QK^t shape: torch.Size([1, 2, 10, 10])\n",
      "Attention weights shape: torch.Size([1, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_heads = 2\n",
    "seq_leng_q = 10\n",
    "d_k = 10\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "key = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "value = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape{value.shape}\")\n",
    "\n",
    "print(f\"Key transposed shape {key.transpose(-2, -1).shape}\")\n",
    "\n",
    "scores = torch.matmul(query, key.transpose(-2, -1)) / d_k ** 0.5\n",
    "print(f\"QK^t shape: {scores.shape}\")\n",
    "\n",
    "attention_weights = nn.Softmax(dim=-1)(scores)\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2, 10, 10])\n",
      "Attention weights shape: torch.Size([1, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Tensor of shape (batch_size, num_heads, seq_len_q, d_k)\n",
    "            key: Tensor of shape (batch_size, num_heads, seq_len_k, d_k)\n",
    "            value: Tensor of shape (batch_size, num_heads, seq_len_v, d_v) \n",
    "                   Typically, seq_len_k = seq_len_v\n",
    "            mask: Tensor of shape (batch_size, 1, 1, seq_len_k)\n",
    "        Returns:\n",
    "            output: Attention values of shape (batch_size, num_heads, seq_len_q, d_v)\n",
    "            attention_weights: Tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the dot products\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        # Apply the mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Compute the attention weights\n",
    "        attention_weights = self.softmax(scores)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Compute the output\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "scaled_dotproduct_attention = ScaledDotProductAttention(d_k)\n",
    "output, attention_weights = scaled_dotproduct_attention.forward(query, key, value)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Multi-Head Attention\n",
    "\n",
    "<img src=\"assets/multi_head_attention.png\" width=\"300\" height=\"400\">\n",
    "\n",
    "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 20, 10])\n",
      "Attention weights shape: torch.Size([1, 2, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism as described in the \"Attention Is All You Need\" paper.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the multi-head attention layer. \n",
    "                       It should be a multiple of num_heads.\n",
    "        num_heads (int): The number of parallel attention layers, or \"heads\".\n",
    "        dropout (float, optional): Dropout rate for the attention weights. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        d_model (int): The dimension of the input and output.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_k (int): Dimension of the key, query, and value for each head.\n",
    "        W_q (nn.Linear): Linear transformation for the query.\n",
    "        W_k (nn.Linear): Linear transformation for the key.\n",
    "        W_v (nn.Linear): Linear transformation for the value.\n",
    "        W_o (nn.Linear): Linear transformation for the output.\n",
    "        attention (ScaledDotProductAttention): The scaled dot product attention mechanism.\n",
    "        dropout (nn.Dropout): Dropout layer for the attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0 # d_model must be divisible by num_heads\n",
    "\n",
    "        self.d_model = d_model          # d_model is the dimension of the model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # d_k is the dimension of the keys and values\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(self.d_k, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension of tensor x into (num_heads, d_k).\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            batch_size (int): Batch size.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Reshaped tensor of shape (batch_size, num_heads, seq_len, d_k).\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor of shape (batch_size, seq_len_q, d_model).\n",
    "            key (torch.Tensor): Key tensor of shape (batch_size, seq_len_k, d_model).\n",
    "            value (torch.Tensor): Value tensor of shape (batch_size, seq_len_v, d_model).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len_k).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len_q, d_model).\n",
    "            torch.Tensor: Attention weights tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k).\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear transformations\n",
    "        query = self.W_q(query)\n",
    "        key = self.W_k(key)\n",
    "        value = self.W_v(value)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # Scale the dot product attention\n",
    "        output, attention_weights = self.attention.forward(query, key, value, mask)\n",
    "\n",
    "        # Concatenate heads and transform\n",
    "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "multihead_attention = MultiHeadAttention(d_model=10, num_heads=num_heads)\n",
    "output, attention_weights = multihead_attention.forward(query, key, value)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "<img src=\"assets/encoder.png\" width=\"250\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single encoder layer in the Transformer architecture.\n",
    "    \n",
    "    Each layer consists of:\n",
    "    1. Multi-head self-attention mechanism.\n",
    "    2. Position-wise feed-forward network.\n",
    "    \n",
    "    Additionally, each of these components is surrounded by a residual connection \n",
    "    followed by layer normalization.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the encoder layer.\n",
    "        num_heads (int): Number of attention heads for the multi-head attention mechanism.\n",
    "        d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
    "        dropout (float, optional): Dropout rate for the attention weights and feed-forward network. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        multihead_attention (MultiHeadAttention): The multi-head attention mechanism.\n",
    "        feed_forward (nn.Sequential): The position-wise feed-forward network.\n",
    "        norm1 (nn.LayerNorm): Layer normalization for the attention mechanism's output.\n",
    "        norm2 (nn.LayerNorm): Layer normalization for the feed-forward network's output.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        # Multi-head attention\n",
    "        attention_output, _ = self.multihead_attention.forward(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # Feed-forward network\n",
    "        feed_forward_output = self.feed_forward.forward(x)\n",
    "        return self.norm2(x + self.dropout(feed_forward_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents the encoder component of the Transformer architecture.\n",
    "    \n",
    "    The encoder consists of a stack of identical layers, where each layer has \n",
    "    a multi-head self-attention mechanism and a position-wise feed-forward network.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the encoder.\n",
    "        num_heads (int): Number of attention heads for the multi-head attention mechanism.\n",
    "        d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
    "        num_layers (int): Number of identical layers in the encoder.\n",
    "        dropout (float, optional): Dropout rate for the attention weights and feed-forward network. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): List of encoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([32, 1, 2, 10])\n",
      "Key shape: torch.Size([32, 1, 2, 10])\n",
      "Value shapetorch.Size([32, 1, 2, 10])\n",
      "Output shape: torch.Size([32, 32, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_heads = 1\n",
    "seq_leng_q = 2    # This is the length of the query (q) sequence.\n",
    "d_k = 10           # This is the dimension of the keys (k), queries (q), and values (v) in the scaled dot-product attention mechanism.\n",
    "d_model = 10       # This is the dimension of the embeddings and the input and output size of the Transformer's encoder and decoder layers.\n",
    "d_ff = 10          # This is the dimension of the hidden layer in the feed-forward network.\n",
    "\n",
    "# The relationship between them is often d_k = d_model / num_heads\n",
    "assert d_model % num_heads == 0 # d_model must be divisible by num_heads\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "key = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "value = torch.rand(batch_size, num_heads, seq_leng_q, d_k)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape{value.shape}\")\n",
    "\n",
    "encoder = TransformerEncoder(d_model=d_model, num_heads=num_heads, d_ff=d_ff, num_layers=1)\n",
    "output = encoder.forward(query)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a classifier using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based classifier.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output of the Transformer encoder.\n",
    "        num_heads (int): Number of attention heads for the multi-head attention mechanism.\n",
    "        d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
    "        num_layers (int): Number of identical layers in the encoder.\n",
    "        num_classes (int): Number of target classes for classification.\n",
    "        dropout (float, optional): Dropout rate for the attention weights and feed-forward network. Default is 0.1.\n",
    "    \n",
    "    Attributes:\n",
    "        encoder (TransformerEncoder): The Transformer encoder.\n",
    "        classifier (nn.Linear): Linear layer for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, num_classes, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "       \"\"\"\n",
    "       Forward pass for the Transformer classifier.\n",
    "       \n",
    "       Args:\n",
    "           x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "           mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, 1, seq_len).\n",
    "           \n",
    "       Returns:\n",
    "           torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
    "       \"\"\"\n",
    "       # Get the encoder's output\n",
    "       encoder_output = self.encoder(x, mask)\n",
    "       logits =  self.classifier(encoder_output)\n",
    "       return self.sigmoid(logits).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_heads = 1\n",
    "seq_leng_q = 1    # This is the length of the query (q) sequence.\n",
    "d_k = 10           # This is the dimension of the keys (k), queries (q), and values (v) in the scaled dot-product attention mechanism.\n",
    "d_model = 10       # This is the dimension of the embeddings and the input and output size of the Transformer's encoder and decoder layers.\n",
    "d_ff = 10   \n",
    "\n",
    "imput_tensor = torch.rand(batch_size, seq_leng_q, d_model)\n",
    "\n",
    "model = TransformerClassifier(d_model=d_model, num_heads=num_heads, d_ff=d_ff, num_layers=1, num_classes=1)\n",
    "output = model.forward(imput_tensor)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imput_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type                  | Params\n",
      "--------------------------------------------------\n",
      "0 | model   | TransformerClassifier | 141   \n",
      "1 | loss_fn | BCELoss               | 0     \n",
      "--------------------------------------------------\n",
      "141       Trainable params\n",
      "0         Non-trainable params\n",
      "141       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a synthetic dataset\n",
    "n_features = \n",
    "X, y = make_classification(n_samples=1000, n_features=n_features, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train, y_val = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "\n",
    "# Define the PyTorch Lightning module\n",
    "class LogisticRegressionModule(pl.LightningModule):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, num_classes):\n",
    "        super(LogisticRegressionModule, self).__init__()\n",
    "        self.model = TransformerClassifier(d_model=d_model, num_heads=num_heads, d_ff=d_ff, num_layers=num_layers, num_classes=num_classes)\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    def on_validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        self.log('val_loss', avg_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 1\n",
    "num_heads = 1\n",
    "seq_leng_q = 1    # This is the length of the query (q) sequence.\n",
    "d_k = 10           # This is the dimension of the keys (k), queries (q), and values (v) in the scaled dot-product attention mechanism.\n",
    "d_model = 10       # This is the dimension of the embeddings and the input and output size of the Transformer's encoder and decoder layers.\n",
    "d_ff = 10          # This is the dimension of the hidden layer in the feed-forward network.\n",
    "\n",
    "\n",
    "# Training the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    )\n",
    "model = LogisticRegressionModule(\n",
    "    d_model=4,\n",
    "    num_heads=1,\n",
    "    d_ff=4,\n",
    "    num_layers=1,\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(d_model=d_model, num_heads=num_heads, d_ff=d_ff, num_layers=1, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x4 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mforward(\u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_loader))[\u001b[39m0\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[28], line 35\u001b[0m, in \u001b[0;36mTransformerClassifier.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mForward pass for the Transformer classifier.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m    torch.Tensor: Output tensor of shape (batch_size, num_classes).\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# Get the encoder's output\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, mask)   \n\u001b[1;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(encoder_output)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 37\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mForward pass for the Transformer encoder.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m    torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 37\u001b[0m     x \u001b[39m=\u001b[39m layer(x, mask)\n\u001b[1;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[25], line 52\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mForward pass for the encoder layer.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m    torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Multi-head attention\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m attention_output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attention\u001b[39m.\u001b[39;49mforward(x, x, x, mask)\n\u001b[1;32m     53\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output))\n\u001b[1;32m     55\u001b[0m \u001b[39m# Feed-forward network\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 69\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     66\u001b[0m batch_size \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Linear transformations\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_q(query)\n\u001b[1;32m     70\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_k(key)\n\u001b[1;32m     71\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_v(value)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x4 and 10x10)"
     ]
    }
   ],
   "source": [
    "model.forward(next(iter(train_loader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
